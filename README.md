Here's a crisp and professional README.md for your CAFFEINE implementation:

```markdown
# CAFFEINE: Computationally-Efficient Federated Unlearning

**CAFFEINE** (Computationally-Efficient Federated Unlearning via First-Order Influence Estimation) is a Hessian-free federated unlearning framework that uses Taylor expansion to approximate influence functions without expensive second-order computations.

## ğŸš€ Key Features

- **Hessian-Free**: Replaces \(O(d^3)\) Hessian inversion with \(O(d)\) gradient computations
- **Taylor Expansion**: First-order approximation: `Î”Î¸ â‰ˆ âˆ‡L(Î¸) - âˆ‡L(Î¸')`
- **Dual Modes**: Supports both centralized and federated learning scenarios
- **Privacy-Preserving**: Local unlearning without requiring other clients' participation
- **Benchmark Ready**: Direct comparison with Ferrari (NeurIPS 2024)

## ğŸ“¦ Installation

```
# Clone repository
git clone https://github.com/yourusername/caffeine-unlearning.git
cd caffeine-unlearning

# Install dependencies
pip install -r requirements.txt
```

**Requirements:**
- Python 3.8+
- PyTorch 2.0+
- torchvision
- numpy, pandas, matplotlib

**GPU Support:**
- NVIDIA GPUs (CUDA)
- Apple Silicon (M1/M2/M3) via MPS (Metal Performance Shaders)
- Automatic device detection

## ğŸ¯ Quick Start

### Centralized Mode (Phase 1)

```
# Train and unlearn on MNIST
python main.py \
    --mode centralized \
    --dataset mnist \
    --train_model \
    --unlearn_ratio 0.1 \
    --output_dir ./results/centralized
```

### Federated Mode (Phase 2)

```
# Federated learning with 10 clients
python main.py \
    --mode federated \
    --dataset cifar10 \
    --num_clients 10 \
    --num_rounds 50 \
    --train_model \
    --client_id 0 \
    --unlearn_ratio 0.1 \
    --output_dir ./results/federated
```

### GPU Acceleration

```bash
# Auto-detect best device (recommended)
python main.py --mode centralized --dataset cifar10 --device auto

# Force NVIDIA GPU
python main.py --mode centralized --dataset cifar10 --device cuda

# Force Apple Silicon
python main.py --mode centralized --dataset cifar10 --device mps

# Force CPU
python main.py --mode centralized --dataset cifar10 --device cpu
```

## ğŸ“ Project Structure

```
caffeine-unlearning/
â”œâ”€â”€ data/                      # Dataset storage
â”œâ”€â”€ datasets/                  # Data loading utilities
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â””â”€â”€ federated_dataset.py
â”œâ”€â”€ models/                    # Model architectures
â”‚   â”œâ”€â”€ cnn.py
â”‚   â””â”€â”€ resnet.py
â”œâ”€â”€ unlearning/               # Core CAFFEINE algorithm
â”‚   â””â”€â”€ caffeine_unlearning.py
â”œâ”€â”€ federated/                # FL components
â”‚   â”œâ”€â”€ server.py
â”‚   â””â”€â”€ client.py
â”œâ”€â”€ main.py                   # Unified entry point
â””â”€â”€ requirements.txt
```

## ğŸ”§ Key Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--mode` | `centralized` or `federated` | `centralized` |
| `--dataset` | Dataset name | `cifar10` |
| `--unlearn_ratio` | Fraction of data to forget | `0.1` |
| `--alpha` | Unlearning strength | `0.1` |
| `--num_iterations` | CAFFEINE iterations | `3` |
| `--lr` | Learning rate for Taylor approximation | `0.001` |
| `--num_clients` | Number of FL clients | `10` |
| `--num_rounds` | FL training rounds | `50` |
| `--device` | Device: `auto`, `cuda`, `mps`, `cpu` | `auto` |

## ğŸ“Š Supported Datasets

- MNIST (28Ã—28 grayscale, 10 classes)
- Fashion-MNIST (28Ã—28 grayscale, 10 classes)
- CIFAR-10 (32Ã—32 RGB, 10 classes)
- CIFAR-100 (32Ã—32 RGB, 100 classes)

## ğŸ§ª Evaluation Metrics

CAFFEINE tracks:
- **Unlearn Effectiveness**: Accuracy drop on forgotten data
- **Retain Preservation**: Accuracy maintenance on retained data
- **Test Utility**: Overall model performance
- **Computational Cost**: Runtime and FLOPs comparison

## ğŸ“– Method Overview

CAFFEINE approximates the influence of removing data point \((x_i, y_i)\) via:

```
Influence â‰ˆ âˆ‡L(Î¸, D_u) - âˆ‡L(Î¸', D_u)
```

where:
- `Î¸` = current model parameters
- `Î¸' = Î¸ - Îµâˆ‡L(Î¸, D_u)` = perturbed parameters
- `D_u` = data to unlearn

**Advantages over Hessian-based methods:**
- No matrix inversion required
- Minimal memory footprint
- Scalable to large models

## ğŸ† Benchmarking

Compare against Ferrari baseline:

```
# Run CAFFEINE
python main.py --mode federated --dataset cifar10 --output_dir ./results/caffeine

# Run Ferrari (from their repo)
python ferrari_main.py --dataset cifar10 --output_dir ./results/ferrari

# Compare results
python compare_results.py --caffeine ./results/caffeine --ferrari ./results/ferrari
```

## ğŸ“„ Citation

If you use CAFFEINE in your research, please cite:

```
@inproceedings{sharma2025caffeine,
  title={CAFFEINE: Computationally-Efficient Federated Unlearning via First-Order Influence Estimation},
  author={Sharma, Neil and [Your Name]},
  booktitle={Proceedings of Middleware Conference},
  year={2025}
}
```

**Benchmark Reference:**
```
@inproceedings{gu2024ferrari,
  title={Ferrari: Federated Feature Unlearning via Optimizing Feature Sensitivity},
  author={Gu, Hanlin and Ong, Win Kent and Chan, Chee Seng and Fan, Lixin},
  booktitle={NeurIPS},
  year={2024}
}
```

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add improvement'`)
4. Push to branch (`git push origin feature/improvement`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Ferrari implementation: [Federated-Feature-Unlearning](https://github.com/OngWinKent/Federated-Feature-Unlearning)
- Computational efficiency approach based on doctoral symposium paper (Middleware 2025)
- Benchmark datasets: MNIST, CIFAR-10/100

## ğŸ“§ Contact

For questions or collaborations:
- GitHub Issues: [Report bugs or request features](https://github.com/yourusername/caffeine-unlearning/issues)
- Email: your.email@university.edu

---

**Status:** ğŸš§ Active Development | **Version:** 0.1.0 | **Last Updated:** November 2025